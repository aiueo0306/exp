import os
import sys
import subprocess
import tempfile
import re
import time
import datetime
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

# ===== GitHub ä¸Šã®å…±é€šé–¢æ•°ã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¯ãƒ­ãƒ¼ãƒ³ =====
REPO_URL = "https://github.com/aiueo0306/shared-python-env.git"
SHARED_DIR = os.path.join(tempfile.gettempdir(), "shared-python-env")

if not os.path.exists(SHARED_DIR):
    print("ğŸ”„ å…±é€šé–¢æ•°ã‚’åˆå›ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­...")
    subprocess.run(["git", "clone", "--depth", "1", REPO_URL, SHARED_DIR], check=True)
else:
    print("ğŸ” å…±é€šé–¢æ•°ã‚’æ›´æ–°ä¸­...")
    subprocess.run(["git", "-C", SHARED_DIR, "pull"], check=True)

sys.path.append(SHARED_DIR)

# ===== å…±é€šé–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ =====
from rss_utils import generate_rss
from scraper_utils import extract_items
from browser_utils import click_button_in_order

# ===== å›ºå®šæƒ…å ±ï¼ˆå­¦ä¼šã‚µã‚¤ãƒˆï¼‰ =====
BASE_URL = "https://medical.taisho.co.jp/medical/doctor-news/"
GAKKAI = "å¤§æ­£è£½è–¬ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ï¼‰"

SELECTOR_TITLE = "div._news_text_jzd1w_19"
title_selector = ""
title_index = 0
href_selector = "a"
href_index = 0
SELECTOR_DATE = "p.mantine-focus-auto._date_jzd1w_1.m_b6d8b162.mantine-Text-root"  # typoä¿®æ­£æ¸ˆã¿
date_selector = ""
date_index = 0
year_unit = "."
month_unit = "."
day_unit = ""
date_format = f"%Y{year_unit}%m{month_unit}%d{day_unit}"
date_regex = rf"(\d{{2,4}}){year_unit}(\d{{1,2}}){month_unit}(\d{{1,2}}){day_unit}"
# date_format = f"%Y{year_unit}%m{month_unit}%d{day_unit}"
# date_regex = rf"(\d{{2,4}}){year_unit}(\d{{1,2}}){month_unit}(\d{{1,2}}){day_unit}"

# ===== ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—é †åºã‚¯ãƒªãƒƒã‚¯è¨­å®š =====
POPUP_MODE = 1  # 0: ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—å‡¦ç†ã—ãªã„, 1: å‡¦ç†ã™ã‚‹
POPUP_BUTTONS = ["è–¬å‰¤å¸«"] if POPUP_MODE else [] 
WAIT_BETWEEN_POPUPS_MS = 500
BUTTON_TIMEOUT_MS = 12000



# ===== Playwright å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ =====
with sync_playwright() as p:
    print("â–¶ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­...")
    browser = p.chromium.launch(headless=True)
    context = browser.new_context(
        locale="ja-JP",
        viewport={"width": 1366, "height": 900},
        user_agent=(
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        extra_http_headers={"Accept-Language": "ja,en;q=0.8"},
    )

    # ğŸ§° è¿½åŠ : Botæ¤œå‡ºã®ç·©å’Œï¼ˆheadlessã§ã‚‚webdriverã‚’undefinedã«ï¼‰
    context.add_init_script(
        "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    )

    # ğŸ§° è¿½åŠ : ãƒ©ãƒ³ãƒŠãƒ¼ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«IPã‚’å‡ºåŠ›ï¼ˆã‚¸ã‚ªãƒ–ãƒ­ãƒƒã‚¯ç–‘ã„ã®åˆ‡ã‚Šåˆ†ã‘ï¼‰
    try:
        ip = context.request.get("https://api.ipify.org?format=json").json()
        print("Runner public IP:", ip)
    except Exception as e:
        print("IP check failed:", e)
    
    page = context.new_page()

     # ğŸ§° è¿½åŠ : ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹/å¤±æ•—ã®ã‚­ãƒ£ãƒ—ãƒãƒ£ï¼ˆè©²å½“APIãŒç©º/403ãªã©åˆ¤å®šï¼‰
    os.makedirs("netlog", exist_ok=True)

    def _tap(url: str) -> bool:
        # â† ã“ã“ã«â€œãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹â€API/ãƒ‘ã‚¹ã®æ–­ç‰‡ã‚’éšæ™‚è¶³ã—ã¦OKï¼ˆæ±ç”¨ã®ã¾ã¾ã§ã‚‚OKï¼‰
        patterns = ["/wp-json/", "/api/", "/ajax", "/doctor-news", "/news", "/wp-admin/admin-ajax.php"]
        return any(s in url for s in patterns)

    def _safe_name(u: str) -> str:
        return re.sub(r"[^a-zA-Z0-9_.-]", "_", u)[:180]

    def on_response(res):
        url = res.url
        if _tap(url):
            try:
                body = res.text()
            except Exception as e:
                body = f"<<read error: {e}>>"
            path = f"netlog/{_safe_name(url)}.txt"
            with open(path, "w", encoding="utf-8") as f:
                f.write(f"STATUS: {res.status}\nURL: {url}\n\n{body}")
            print("ğŸ“¥ captured:", path)

    def on_request_failed(req):
        if _tap(req.url):
            print("âŒ request failed:", req.url, req.failure)

    page.on("response", on_response)
    page.on("requestfailed", on_request_failed)
    
    try:
        print("â–¶ ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
        page.goto(BASE_URL, timeout=30000)
        
        try:
            page.wait_for_load_state("networkidle", timeout=120000)
            print("é€šéã—ãŸã‚ˆ")
        except Exception:
            page.wait_for_load_state("domcontentloaded")
        print("ğŸŒ åˆ°é”URL:", page.url)

        # ---- ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—é †ã«å‡¦ç†ï¼ˆPOPUP_MODE ãŒ 1 ã®ã¨ãã ã‘å®Ÿè¡Œï¼‰----
        if POPUP_MODE and POPUP_BUTTONS:
            for i, label in enumerate(POPUP_BUTTONS, start=1):
                handled = click_button_in_order(page, label, step_idx=i, timeout_ms=BUTTON_TIMEOUT_MS)
                if handled:
                    page.wait_for_timeout(WAIT_BETWEEN_POPUPS_MS)
                else:
                    # å‡ºãªã„æ—¥ã‚‚ã‚ã‚‹ã‚µã‚¤ãƒˆãªã‚‰ 'continue' ã«å¤‰æ›´
                    break
        else:
            print("â„¹ ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆPOPUP_MODE=0 ã¾ãŸã¯ ãƒœã‚¿ãƒ³æœªæŒ‡å®šï¼‰")

        # ğŸ§° è¿½åŠ : ãƒ­ãƒ¼ãƒ«/èªè¨¼ã®ãƒ’ãƒ³ãƒˆã«ãªã‚Šãã†ãªæƒ…å ±ã‚’åãï¼ˆCookie & localStorageï¼‰
        try:
            role_info = page.evaluate("() => ({ls: {...localStorage}, ck: document.cookie})")
            print("role/localStorage snapshot:", role_info)
        except Exception as e:
            print("role snapshot failed:", e)

        # ğŸ§° è¿½åŠ : IntersectionObserverç³»ã®é…å»¶ãƒ­ãƒ¼ãƒ‰ã«å‚™ãˆã¦ â€œè¦‹ã›ã‚‹â€ å‹•ã
        try:
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(600)
            page.evaluate("window.scrollTo(0, 0)")
        except Exception as e:
            print("scroll nudge failed:", e)
        
        # æœ¬æ–‡èª­ã¿è¾¼ã¿
        page.wait_for_load_state("load", timeout=30000)

    except PlaywrightTimeoutError:
        print("âš  ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        browser.close()
        raise

    try:
       page.wait_for_selector(SELECTOR_TITLE, state="attached", timeout=30000)
    except Exception as e:
        print("âš ï¸ è¦ç´ å¾…ã¡ã§ã‚¨ãƒ©ãƒ¼:", e)
            # é€”ä¸­çŠ¶æ…‹ã‚’å¿…ãšä¿å­˜
        save_dir = os.getcwd()
        html_path = os.path.join(save_dir, "page.html")
        screenshot_path = os.path.join(save_dir, "screenshot.png")
        
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(page.content())
        page.screenshot(path=screenshot_path, full_page=True)
        
        print("ğŸ’¾ ã‚¨ãƒ©ãƒ¼æ™‚ã«ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«:", html_path, screenshot_path)
        # ã‚¨ãƒ©ãƒ¼ã¯å†é€å‡ºã—ã¦å‡¦ç†çµ‚äº†
        raise
    
    print("â–¶ è¨˜äº‹ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")    
    items = extract_items(
        page,
        SELECTOR_DATE,
        SELECTOR_TITLE,
        title_selector,
        title_index,
        href_selector,
        href_index,
        BASE_URL,
        date_selector,
        date_index,
        date_format,
        date_regex,
    )

    if not items:
        print("âš  æŠ½å‡ºã§ããŸè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚HTMLæ§‹é€ ãŒå¤‰ã‚ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    os.makedirs("rss_output", exist_ok=True)
    rss_path = "rss_output/Feed20-2.xml"
    generate_rss(items, rss_path, BASE_URL, GAKKAI)
    browser.close()
