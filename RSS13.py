import os
import sys
import subprocess
import tempfile
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

# ===== GitHub ä¸Šã®å…±é€šé–¢æ•°ã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¯ãƒ­ãƒ¼ãƒ³ =====
REPO_URL = "https://github.com/aiueo0306/shared-python-env.git"
SHARED_DIR = os.path.join(tempfile.gettempdir(), "shared-python-env")

if not os.path.exists(SHARED_DIR):
    print("ğŸ”„ å…±é€šé–¢æ•°ã‚’åˆå›ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­...")
    subprocess.run(["git", "clone", "--depth", "1", REPO_URL, SHARED_DIR], check=True)
else:
    print("ğŸ” å…±é€šé–¢æ•°ã‚’æ›´æ–°ä¸­...")
    subprocess.run(["git", "-C", SHARED_DIR, "pull"], check=True)

sys.path.append(SHARED_DIR)

# ===== å…±é€šé–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ =====
from rss_utils import generate_rss
from scraper_utils import extract_items

BASE_URL = "https://www.dermatol.or.jp/modules/newslist/index.php?content_id=1"
GAKKAI = "æ—¥æœ¬çš®è†šç§‘å­¦ä¼š"

SELECTOR_TITLE = "div#right_column div.news_title"
title_selector = "a"
title_index = 0
href_selector = "a"
href_index = 0
SELECTOR_DATE = "div#right_column div.news_date"
date_selector = ""  
date_index = 0       
year_unit = "å¹´"
month_unit = "æœˆ"
day_unit = "æ—¥"  
date_format = f"%Y{year_unit}%m{month_unit}%d{day_unit}"
date_regex = rf"(\d{{2,4}}){year_unit}(\d{{1,2}}){month_unit}(\d{{1,2}}){day_unit}"

# ===== å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ =====
with sync_playwright() as p:
    print("â–¶ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­...")
    browser = p.chromium.launch(headless=True)
    context = browser.new_context()
    page = context.new_page()

    try:
        print("â–¶ ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
        page.goto(BASE_URL, timeout=240000)
        try:
            page.wait_for_load_state("networkidle", timeout=240000)
        except Exception:
            page.wait_for_load_state("domcontentloaded")
        page.wait_for_load_state("load", timeout=240000)
    except PlaywrightTimeoutError:
        print("âš  ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        browser.close()
        exit()

    print("â–¶ è¨˜äº‹ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
    items = extract_items(
    page,
    SELECTOR_DATE,
    SELECTOR_TITLE,
    title_selector,
    title_index,
    href_selector,
    href_index,
    BASE_URL,
    date_selector,    
    date_index,      
    date_format, 
    date_regex, 
    )

    if not items:
        print("âš  æŠ½å‡ºã§ããŸè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚HTMLæ§‹é€ ãŒå¤‰ã‚ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    rss_path = "rss_output/Feed10.xml"
    generate_rss(items, rss_path, BASE_URL, GAKKAI)
    browser.close()
